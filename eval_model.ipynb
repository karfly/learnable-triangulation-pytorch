{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annual-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import copy\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from mvn.models.triangulation import RANSACTriangulationNet, AlgebraicTriangulationNet, VolumetricTriangulationNet\n",
    "from mvn.models.loss import KeypointsMSELoss, KeypointsMSESmoothLoss, KeypointsMAELoss, KeypointsL2Loss, VolumetricCELoss, element_weighted_loss\n",
    "\n",
    "from mvn.utils import img, multiview, op, vis, misc, cfg\n",
    "from mvn.datasets import human36m\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "\n",
    "from mvn.utils.minimon import MiniMon\n",
    "from mvn.utils.misc import normalize_transformation, flush_cache\n",
    "from mvn.utils.dicts import NestedNamespace\n",
    "\n",
    "from train import setup_dataloaders, setup_experiment\n",
    "\n",
    "\n",
    "flush_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latter-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# available GPUs: 1\n",
      "estimating dataset size ...\n",
      "  ... available subjects ['S1', 'S6', 'S7', 'S8'] and subsampling 1/10000 => 12 available frames\n",
      "estimating dataset size ...\n",
      "  ... available subjects ['S9', 'S11'] and subsampling 1/100 => 22 available frames\n",
      "  training dataset length: 12\n",
      "  validation dataset length: 5\n"
     ]
    }
   ],
   "source": [
    "def get_args():\n",
    "    args = NestedNamespace(\n",
    "        dict(\n",
    "            config='experiments/human36m/train/human36m_alg.yaml',\n",
    "            eval=False,\n",
    "            eval_dataset='val',\n",
    "            local_rank=None, logdir='/home/stefano/_tmp/logs',\n",
    "            seed=42\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print('# available GPUs: {:d}'.format(torch.cuda.device_count()))\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def get_config(args):\n",
    "    config = cfg.load_config(args.config)\n",
    "\n",
    "    config.image_shape = [128, 128]\n",
    "    config.debug.write_imgs = True\n",
    "    config.debug.img_out = '/home/stefano/_tmp/logs/imgs'\n",
    "    config.debug.dump_checkpoints = False\n",
    "\n",
    "    config.opt.n_epochs = 2\n",
    "    config.opt.n_iters_per_epoch = config.opt.n_objects_per_epoch // config.opt.batch_size\n",
    "    \n",
    "    config.opt.torch_anomaly_detection = False\n",
    "\n",
    "    config.opt.loss_3d = False\n",
    "    config.opt.loss_2d = not config.opt.loss_3d\n",
    "    config.opt.batch_size = 8\n",
    "    config.opt.val_batch_size = 16\n",
    "\n",
    "    data_folder = '/home/stefano/_tmp/data/'\n",
    "\n",
    "    config.model.triangulate_in_world_space = False\n",
    "    config.model.triangulate_in_cam_space = False\n",
    "    config.model.cam2cam_estimation = True\n",
    "    config.model.init_weights = False  # there is no point in loading full module with a shitty GPU\n",
    "    config.model.checkpoint = data_folder + 'weights_alg.pth'  #  + 'weights_vol.pth'\n",
    "    config.model.backbone.checkpoint = data_folder + 'pose_resnet_4.5_pixels_human36m.pth'\n",
    "    config.model.backbone.init_weights = config.model.init_weights\n",
    "    config.model.backbone.num_layers = 18  # very small BB\n",
    "    config.model.backbone.num_deconv_filters = 32\n",
    "\n",
    "    config.dataset.train.h36m_root = data_folder + 'processed/'\n",
    "    config.dataset.train.labels_path = data_folder + 'human36m-multiview-labels-GTbboxes.npy'\n",
    "    config.dataset.train.retain_every_n_frames_in_train = 10000  # 12 images when in full dataset\n",
    "    config.dataset.train.num_workers = 2\n",
    "\n",
    "    config.dataset.val.h36m_root = config.dataset.train.h36m_root  # the same! WTF!\n",
    "    config.dataset.val.labels_path = config.dataset.train.labels_path  # the same! WTF!\n",
    "    config.dataset.val.retain_every_n_frames_in_test = 500  # 5 images when in full dataset\n",
    "    config.dataset.val.num_workers = 2\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def build_labels(f_path, retain_every_n_frames, allowed_subjects=['S1', 'S6', 'S7', 'S8']):\n",
    "    print('estimating dataset size ...')\n",
    "    labels = np.load(f_path, allow_pickle=True).item()\n",
    "    \n",
    "    subjects = [\n",
    "        labels['subject_names'].index(x)\n",
    "        for x in allowed_subjects  # todo solve missing images in 'S5'\n",
    "    ]\n",
    "    \n",
    "    mask = np.isin(labels['table']['subject_idx'], subjects, assume_unique=True)\n",
    "    indices = np.nonzero(mask)[0][::retain_every_n_frames]\n",
    "    \n",
    "    print('  ... available subjects {} and subsampling 1/{:d} => {:d} available frames'.format(\n",
    "        allowed_subjects,\n",
    "        retain_every_n_frames,\n",
    "        len(indices)\n",
    "    ))\n",
    "    \n",
    "    return labels, mask, indices\n",
    "\n",
    "\n",
    "def build_env(config, device):\n",
    "    model = {\n",
    "        \"ransac\": RANSACTriangulationNet,\n",
    "        \"alg\": AlgebraicTriangulationNet,\n",
    "        \"vol\": VolumetricTriangulationNet\n",
    "    }[config.model.name](config, device=device).to(device)\n",
    "\n",
    "    if config.model.init_weights:\n",
    "        state_dict = torch.load(config.model.checkpoint)\n",
    "        for key in list(state_dict.keys()):\n",
    "            new_key = key.replace(\"module.\", \"\")\n",
    "            state_dict[new_key] = state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        print('Successfully loaded pretrained weights for whole model')\n",
    "    \n",
    "    criterion_class = {\n",
    "        \"MSE\": KeypointsMSELoss,\n",
    "        \"MSESmooth\": KeypointsMSESmoothLoss,\n",
    "        \"MAE\": KeypointsMAELoss\n",
    "    }[config.opt.criterion]\n",
    "\n",
    "    if config.opt.criterion == \"MSESmooth\":\n",
    "        criterion = criterion_class(config.opt.mse_smooth_threshold)\n",
    "    else:\n",
    "        criterion = criterion_class()\n",
    "\n",
    "    if config.model.name == \"vol\":\n",
    "        opt = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    'params': model.backbone.parameters()\n",
    "                },\n",
    "                {\n",
    "                    'params': model.process_features.parameters(),\n",
    "                    'lr': config.opt.process_features_lr if hasattr(config.opt, \"process_features_lr\") else config.opt.lr\n",
    "                },\n",
    "                {\n",
    "                    'params': model.volume_net.parameters(),\n",
    "                    'lr': config.opt.volume_net_lr if hasattr(config.opt, \"volume_net_lr\") else config.opt.lr\n",
    "                }\n",
    "            ],\n",
    "            lr=config.opt.lr\n",
    "        )\n",
    "    else:\n",
    "        opt = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=config.opt.lr\n",
    "        )\n",
    "    \n",
    "    return model, criterion, opt\n",
    "\n",
    "\n",
    "is_distributed = False\n",
    "master = True\n",
    "device = torch.device(0)\n",
    "    \n",
    "args = get_args()\n",
    "config = get_config(args)\n",
    "model, criterion, opt = build_env(config, torch.device(0))\n",
    "labels, mask, indices = build_labels(config.dataset.train.labels_path, 10000)\n",
    "labels, mask, indices = build_labels(config.dataset.train.labels_path, 100, allowed_subjects=['S9', 'S11'])\n",
    "\n",
    "train_dataloader, val_dataloader, train_sampler = setup_dataloaders(config, distributed_train=is_distributed)  # ~ 0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedicated-speech",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 has started!\n",
      "torch.Size([8, 6, 2, 17, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,3) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d226d999ff78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mminimon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mminimon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/train.py\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(model, criterion, opt, config, dataloader, device, epoch, minimon, is_train, master, experiment_dir)\u001b[0m\n\u001b[1;32m    520\u001b[0m                     )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 results_pred = iter_batch(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/train.py\u001b[0m in \u001b[0;36miter_batch\u001b[0;34m(batch, iter_i, model, model_type, criterion, opt, config, dataloader, device, epoch, minimon, is_train)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcam2cam_estimation\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# predict cam2cam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         results = cam2cam_iter(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/train.py\u001b[0m in \u001b[0;36mcam2cam_iter\u001b[0;34m(batch, iter_i, model, model_type, criterion, opt, images_batch, is_train, config, minimon)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     ]  # I only need [(0, 0)=None, (0, 1), (0, 2), (0, 3)] since 0 is the master\n\u001b[0;32m--> 440\u001b[0;31m     cam2cams = torch.FloatTensor([\n\u001b[0m\u001b[1;32m    441\u001b[0m         [\n\u001b[1;32m    442\u001b[0m             dataset_utils.cam2cam_precomputed_batch(\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/train.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    439\u001b[0m     ]  # I only need [(0, 0)=None, (0, 1), (0, 2), (0, 3)] since 0 is the master\n\u001b[1;32m    440\u001b[0m     cam2cams = torch.FloatTensor([\n\u001b[0;32m--> 441\u001b[0;31m         [\n\u001b[0m\u001b[1;32m    442\u001b[0m             dataset_utils.cam2cam_precomputed_batch(\n\u001b[1;32m    443\u001b[0m                 \u001b[0mmaster_cams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cameras'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam2cam_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/train.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    440\u001b[0m     cam2cams = torch.FloatTensor([\n\u001b[1;32m    441\u001b[0m         [\n\u001b[0;32m--> 442\u001b[0;31m             dataset_utils.cam2cam_precomputed_batch(\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0mmaster_cams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cameras'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam2cam_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             )\n",
      "\u001b[0;32m~/scratch/learnable-triangulation-pytorch/mvn/datasets/utils.py\u001b[0m in \u001b[0;36mcam2cam_precomputed_batch\u001b[0;34m(src, target, cameras_batch, batch_i, cam2cams)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintrinsics_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     return cameras_batch[target][batch_i].intrinsics_padded.dot(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mcam2cams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# cam2cam from src -> target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,4) and (3,3) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "from train import one_epoch\n",
    "\n",
    "minimon = MiniMon()\n",
    "for epoch in range(config.opt.n_epochs):\n",
    "    # cannot use train_sampler.set_epoch since we're not distributed !\n",
    "    \n",
    "    f_out = 'epoch {:4d} has started!'\n",
    "    print(f_out.format(epoch))\n",
    "\n",
    "    minimon.enter()\n",
    "    one_epoch(model, criterion, opt, config, train_dataloader, device, epoch, minimon, is_train=True, master=master)\n",
    "    minimon.leave('train epoch')\n",
    "\n",
    "    minimon.enter()\n",
    "    one_epoch(model, criterion, opt, config, val_dataloader, device, epoch, minimon, is_train=False, master=master)\n",
    "    minimon.leave('eval epoch')\n",
    "\n",
    "    f_out = 'epoch {:4d} complete!'\n",
    "    print(f_out.format(epoch))\n",
    "    \n",
    "minimon.print_stats(as_minutes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 3)\n",
    "b = torch.rand(3, 3)\n",
    "\n",
    "torch.cat((a.unsqueeze(0), b.unsqueeze(0)), dim=0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnable-triangulation-pytorch",
   "language": "python",
   "name": "learnable-triangulation-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
